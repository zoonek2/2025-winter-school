{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93e07d3",
   "metadata": {},
   "source": [
    "# Code to generate the plots used in the presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv venv \n",
    "# uv pip install pandas matplotlib seaborn scipy scikit-learn statsmodels palmerpenguins pip ipykernel ipywidgets setuptools jinja2 transformers tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379e09d-9b1a-41c0-af11-decfcdd52355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import cycle\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca13a1-c417-4b45-b6b0-24a8d145ce5a",
   "metadata": {},
   "source": [
    "# Next word probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1d021-c712-473e-86b8-913b59ef116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to change the text to complete\n",
    "text = \"It was the best of times,\"\n",
    "text = \"The wizard\" \n",
    "\n",
    "# Code adapted from: https://stackoverflow.com/questions/76397904/generate-the-probabilities-of-all-the-next-possible-word-for-a-given-text\n",
    "\n",
    "t = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "m = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "fig, axs = plt.subplots( 3, 6, figsize = (16,9), layout = 'constrained', dpi = 100 )\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "        \n",
    "    encoded_text = t(text, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "      outputs = m(**encoded_text)\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    next_token_probs = torch.softmax(next_token_logits, -1)\n",
    "    topk_next_tokens= torch.topk(next_token_probs, 10)\n",
    "    \n",
    "    next_word = pd.DataFrame( \n",
    "        [(t.decode(idx), prob.item()) for idx, prob in zip(topk_next_tokens.indices, topk_next_tokens.values)], \n",
    "        columns = ['token', 'probability'],\n",
    "    )\n",
    "    next_word['probability'] /= next_word['probability'].sum()\n",
    "    next_word = next_word.iloc[::-1,:]\n",
    "\n",
    "    w = np.random.choice( next_word['token'], p = next_word['probability'] )   # Random token\n",
    "    #w = next_word['token'].iloc[-1]                                            # Most likely token\n",
    "\n",
    "    if i <  len(axs.flatten()): \n",
    "            \n",
    "        #fig, ax = plt.subplots( figsize = (4,4), layout = 'constrained' )\n",
    "        ax = axs.flatten()[i]\n",
    "        b = ax.barh( next_word['token'], next_word['probability'] )\n",
    "        j = np.argwhere( next_word['token'] == w )[0,0]\n",
    "        b[j].set_color('tab:red')\n",
    "        for side in ['left', 'top', 'right']: \n",
    "            ax.spines[side].set_visible(False)\n",
    "        ax.tick_params(axis='y', length=0)\n",
    "        ax.set_xlim(0,1)\n",
    "        if i < 3:  # axs.shape[1]: \n",
    "            ax.set_title( re.sub(r'\\s+', ' ', text) )\n",
    "        #plt.show()\n",
    "\n",
    "    text += w\n",
    "\n",
    "plt.show()\n",
    "print( text )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa0213-54b2-4d65-954e-3043724edf98",
   "metadata": {},
   "source": [
    "# Text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312ca84-1351-4e87-8488-0d8ba053b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_new_tokens=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f5870-3628-4eeb-956a-290199a05205",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def90975-6b33-4370-a6db-14d6ea8f5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"It was the best of times, it was the worst of times, it was the age of\n",
    "wisdom, it was the age of foolishness, it was the epoch of belief, it\n",
    "was the epoch of incredulity, it was the season of Light, it was the\n",
    "season of Darkness, it was the spring of hope, it was the winter of\n",
    "despair, we had everything before us, we had nothing before us, we were\n",
    "all going direct to Heaven, we were all going direct the other way--in\n",
    "short, the period was so far like the present period, that some of its\n",
    "noisiest authorities insisted on its being received, for good or for\n",
    "evil, in the superlative degree of comparison only.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af07410-9f63-4f2f-af58-78f34d1fd399",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = \"\"\"<html>\n",
    "<head>\n",
    "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Alegreya\">\n",
    "<style>\n",
    "body {\n",
    "  font-family: \"Alegreya\", serif;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\"\"\"\n",
    "\n",
    "after = \"\"\"</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "text = text.replace( \"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252cc9f3-e8f0-4e34-9b07-61a78aebcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "token_ids = tokenizer(text)['input_ids']\n",
    "tokens = [ tokenizer.decode(u) for u in token_ids ]\n",
    "colors = cycle( [ 'blue', 'red' ] )\n",
    "colors = cycle( ['Aqua', 'Bisque', 'LightGreen', 'LightSalmon', 'SkyBlue', 'Pink' ] )\n",
    "''.join( [ f'<span style=\"color:{color}\">{text}</span>' for text, color in zip( tokens, colors ) ] )\n",
    "result = ''.join( [ f'<span style=\"background:{color}\">{text}</span>' for text, color in zip( tokens, colors ) ] )\n",
    "with open('a.html','w') as f:\n",
    "    print( \n",
    "        before + \n",
    "        result +\n",
    "        after,\n",
    "        file = f,\n",
    "    )\n",
    "! perl -p -e 's#</?span.*?>##g' a.html  > b.html  \n",
    "HTML( f\"<span style='font-size: 2em'>\" + result + \"</span>\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecaba4-6daa-42c6-bc49-b7fe03ab6466",
   "metadata": {},
   "source": [
    "# Token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea291bb-0252-462e-927d-55d22d40b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ec4d8-eba9-4246-9ea2-9551353abe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.wte( torch.tensor( token_ids ) )\n",
    "for i in range(20): \n",
    "    a = f'\"{tokens[i]}\"'\n",
    "    a = f'{a:10s}' + str([ round(u,3) for u in x[i,:10].tolist() ] + [\"⋯\"])\n",
    "    a = a.replace( \"'⋯'\", \"⋯\" )\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca3dc3-d0cd-4cb5-8512-72af8c574014",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"central bank\"\n",
    "token_ids = tokenizer(text)['input_ids']\n",
    "tokens = [ tokenizer.decode(u) for u in token_ids ]\n",
    "x = model.wte( torch.tensor( token_ids ) )\n",
    "for i in range(2): \n",
    "    a = f'\"{tokens[i]}\"'\n",
    "    a = f'{a:10s}' + str([ round(u,3) for u in x[i,:10].tolist() ] + [\"⋯\"])\n",
    "    a = a.replace( \"'⋯'\", \"⋯\" )\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f62400-a196-4f05-8f22-da5c96e25af8",
   "metadata": {},
   "source": [
    "# Plots used in the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836ed5b-2ef0-4062-b5b4-3c095eec9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots( 2, 3, figsize = (15, 8), dpi = 100 )\n",
    "\n",
    "a = 3\n",
    "xs = np.linspace( -a, a, 101 )\n",
    "\n",
    "# Linear\n",
    "ax = axs[0,0]\n",
    "ys = 1 + 1.5 * xs\n",
    "ax.plot( xs, ys, linewidth = 5, zorder = 10 )\n",
    "\n",
    "# ReLU\n",
    "ax = axs[0,1]\n",
    "ys = np.maximum( xs, 0 )\n",
    "ax.plot( xs, ys, linewidth = 5, zorder = 10 )\n",
    "\n",
    "# Leaky ReLU\n",
    "ax = axs[0,2]\n",
    "ys = np.where( xs >= 0, ys, .2 * xs )\n",
    "ax.plot( xs, ys, linewidth = 5, zorder = 10 )\n",
    "\n",
    "# Softplus\n",
    "ax = axs[1,0]\n",
    "ys = np.log( 1 + np.exp(xs) )\n",
    "ax.plot( xs, ys, linewidth = 5, zorder = 10 )\n",
    "\n",
    "# tanh\n",
    "ax = axs[1,1]\n",
    "ys = np.tanh( xs )\n",
    "ax.plot( xs, ys, linewidth = 5, zorder = 10 )\n",
    "\n",
    "ax = axs[1,2]\n",
    "ys = 1 / ( 1 + np.exp(-xs) )\n",
    "ax.plot( xs, ys, linewidth = 5, zorder = 10 )\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.axhline( 0, color = 'black' )\n",
    "    ax.axvline( 0, color = 'black' )\n",
    "    ax.scatter( np.arange(-a,a+1), (2*a+1)*[0], marker = '+', color = 'black' )\n",
    "    ax.scatter( (2*a+1)*[0], np.arange(-a,a+1), marker = '+', color = 'black' )\n",
    "    ax.set_xlim( -1.02*a, 1.02*a )\n",
    "    ax.set_ylim( -1.02*a, 1.02*a )\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
